---
toc: false
layout: post
description: Boosting
categories: [markdown]
title: Some thoughts on boosting
---

# Ensembling

I want to start the series of technical posts with something really simple that turns out to be
a popular ideal in modeling datasets: boosting. One of the incarnation of this idea, XGBoost,
is a very popular tool used in data modelling (see the paper [^XGBoost] the [documentation](https://xgboost.readthedocs.io/en/latest/) of the library)
In a nutshell, it is designed to drive feature discovery in the data by aggregating the insights
generated by a sequence of smaller models, _trained sequentially_ to discover and adapt to the
shape of the training data. While deep learning methods (e.g. CNN-like architectures) are useful
in analysing data that have specific structure (images, time series etc.) as they extract
higher level features, the fact that they are data hungry and the implicit assumptions made on
the structure and correlation on the various features make them not necessarily the first class
of methods one should use.

Boosting belongs to a broader set of techniques going under the name of _ensembling_, which
tries to achieve performance not by refining a single model, but by combining different models
trained in different conditions and with different purposes.
This aggregation is performed so that the weaknesses of a single model are reduced and a more accurate model is generated. Typically the base model $g(x; \theta)$ is taken to be a relatively simple model (like a shallow decision tree), with limited power, which is commonly called a __weak learner__.
The art of ensembling consists in the organization of a set of weak learners such that the prediction
of the ensemble has a better modelling performance than a single, complex learner. I will
refer to [^Dietterich2000] [^Dietterich2002] and [^Murphy] for a more thorough exposition 
and additional pointers to the literature.


## Bagging

One type of model ensembling is __bagging__ (bootstrap aggregating), where an ensemble is built by training weak learners on
different subsets of the original datasets, with the prediction of the full model consisting into a suitable aggregation of the outputs of each submodel. The idea behind this technique is that, __if__ the learners
are performing slightly better than random __and if__ the learners can be considered statistically independent (e.g. they are trained on non-overlapping subsets of the original dataset, either in terms
of samples included or in terms of features used), the prediction
of the ensemble is superior.

In a classification context, for instance, this can be understood by a simple reasoning. If the models are independent (i.e. their misclassifications are uncorrelated, on average) the
probability that the majority of them is misclassifying is much smaller than the probability of misclassification of a single model.

Bagging allows for parallel training of the weak learners, which can be trained on different
partitions of the dataset and using different features.
An example of bagging ensemble is the case of Random Forests of Decision Trees.

## Additive models: Boosting

Bagging essentially leverages on the fact that a set of weak learners trained on a dataset will be
able to learn the structure of the dataset on average, with different learners capturing different
relationships between features and outputs as present in the data. A different approach consists
into training __sequentially__ a set of weak learners in such a way that the aggregate additive model
is able to capture more sophisticated features. This is done by iteratively adapting the
model by adding at each iteration a new component that tries to learn what previous iterations
have missed. In discussing this technique I will focus on the case of regression, instead of
classification, as it is more transparent, in my opinion.

Instead of training a powerful model (which would presumably require a large number
of data points to avoid to simply memorize the dataset) one considers the general class of models

$
G(x; {\theta\_k}\_{k=1}^S) = \sum\_{k=1}^{S} w\_k g(x; \theta\_k)
$

The idea is to _progressively_ adapt the weak learners so that the expressivity of the aggregate
model is gradually adapted to the data.

The idea is pretty simple. At each iteration, compute the gradient with respect to the model
of the loss function evaluated at the current iteration. That gradient represents the linear
approximation of the change to the model that would ensure a lower loss, a sort of gradient
descent in function space (then restricted to a particular parametric form). In the case of
the regression problem with MSE loss, the gradient is just the vector of the residuals, with indices being
the data points of the training set. The gist of the algorithm consists in just training a new learner
to match the current residuals and then to add it to the current model. Again, see [^Murphy],
chapter 16, for further discussion.

## Comments

The fact that the performance on a given dataset is statistically better, in terms of some suitable measure, does not mean that the model should be accepted without questions. For instance, training a random forest
on a dataset representing how a quantity depends on another might lead to a model with a better performance when statistical errors are considered, but with a poor representation of the function class to which the relationship belongs (e.g. the model might produce a piecewise continuous function instead
of a smooth one).

Also, boosting is a sort of midway between a parametric model and a nonparametric one, in the
sense that while a parametric family of learner is chosen, the model itself is adapting to the
data by expanding the space of functions that can be represented beyond what is allowed by
the original weak learner. This is rather obvious from the fact that the model used is
_additive_ in the weak learners, and that each stage of the construction is designed to
capture different aspects of the dataset which cannot be captured in a single pass in which
all the learners are exposed to the dataset. This also explains the power of these methods.

## Why is this interesting?

Apart from having used XGBoost in the past, I have been recently playing with the problem of
reconstructing the geometry of the expanding universe we live in, given observational data.
The goal is to have a flexible tool that is automatically adapting to the small number of data
points available. GPs work well and they are currently used in some areas of Cosmology, but
it is interesting to consider other options that can be used in a regime in which very few
points are available and yet the desired model should be expressive enough not to inject
approximations that are too brutal and not very faithful to the data.
I was looking in particular at the data presented in the recent paper [^BernardoSaid], see
the notebooks on GitHub [here](https://github.com/reggiebernardo/notebooks/tree/main/supp_ntbks_arxiv.2105.12970), where
Gaussian processes are used to model possible gravitational theories.

It is clear that while
boosted trees are certainly an option, they fail to include the necessary degree of smoothness
that is implicitly required for the functions for which an interpolation is sought, and some
other type of weak learner should be used. Alternatively, the choice of Gaussian Processes
with suitable assumptions on the kernel is the best course of action, as it makes no
commitment on the parametric form of the model, while retaining the ability of controlling
smoothness of the class of functions that the model targets.

## References

[^XGBoost] Chen, T. and Guestrin, C., 2016, August. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining (pp. 785-794).

[^Murphy] Murphy, Kevin P. Machine learning: a probabilistic perspective. MIT press, 2012.

[^Dietterich2000] Dietterich, Thomas G. "Ensemble methods in machine learning." International workshop on multiple classifier systems. Springer, Berlin, Heidelberg, 2000.

[^Dietterich2002] Dietterich, Thomas G. "Ensemble learning." The handbook of brain theory and neural networks 2 (2002): 110-125.

[^BernardoSaid] Bernardo, Reginald Christian, and Jackson Levi Said. "A data-driven Reconstruction of Horndeski gravity via the Gaussian processes." arXiv preprint arXiv:2105.12970 (2021). (https://arxiv.org/abs/2105.12970)[https://arxiv.org/abs/2105.12970]
