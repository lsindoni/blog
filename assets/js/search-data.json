{
  
    
        "post0": {
            "title": "Some thoughts on boosting",
            "content": "Ensembling . I want to start the series of technical posts with something really simple that turns out to be a popular ideal in modeling datasets: boosting. One of the incarnation of this idea, XGBoost, is a very popular tool used in data modelling (see the paper [^XGBoost] the documentation of the library) In a nutshell, it is designed to drive feature discovery in the data by aggregating the insights generated by a sequence of smaller models, trained sequentially to discover and adapt to the shape of the training data. While deep learning methods (e.g. CNN-like architectures) are useful in analysing data that have specific structure (images, time series etc.) as they extract higher level features, the fact that they are data hungry and the implicit assumptions made on the structure and correlation on the various features make them not necessarily the first class of methods one should use. . Boosting belongs to a broader set of techniques going under the name of ensembling, which tries to achieve performance not by refining a single model, but by combining different models trained in different conditions and with different purposes. This aggregation is performed so that the weaknesses of a single model are reduced and a more accurate model is generated. Typically the base model $g(x; theta)$ is taken to be a relatively simple model (like a shallow decision tree), with limited power, which is commonly called a weak learner. The art of ensembling consists in the organization of a set of weak learners such that the prediction of the ensemble has a better modelling performance than a single, complex learner. I will refer to [^Dietterich2000] [^Dietterich2002] and [^Murphy] for a more thorough exposition and additional pointers to the literature. . Bagging . One type of model ensembling is bagging (bootstrap aggregating), where an ensemble is built by training weak learners on different subsets of the original datasets, with the prediction of the full model consisting into a suitable aggregation of the outputs of each submodel. The idea behind this technique is that, if the learners are performing slightly better than random and if the learners can be considered statistically independent (e.g. they are trained on non-overlapping subsets of the original dataset, either in terms of samples included or in terms of features used), the prediction of the ensemble is superior. . In a classification context, for instance, this can be understood by a simple reasoning. If the models are independent (i.e. their misclassifications are uncorrelated, on average) the probability that the majority of them is misclassifying is much smaller than the probability of misclassification of a single model. . Bagging allows for parallel training of the weak learners, which can be trained on different partitions of the dataset and using different features. An example of bagging ensemble is the case of Random Forests of Decision Trees. . Additive models: Boosting . Bagging essentially leverages on the fact that a set of weak learners trained on a dataset will be able to learn the structure of the dataset on average, with different learners capturing different relationships between features and outputs as present in the data. A different approach consists into training sequentially a set of weak learners in such a way that the aggregate additive model is able to capture more sophisticated features. This is done by iteratively adapting the model by adding at each iteration a new component that tries to learn what previous iterations have missed. In discussing this technique I will focus on the case of regression, instead of classification, as it is more transparent, in my opinion. . Instead of training a powerful model (which would presumably require a large number of data points to avoid to simply memorize the dataset) one considers the general class of models . $ G(x; { theta_k}_{k=1}^S) = sum_{k=1}^{S} w_k g(x; theta_k) $ . The idea is to progressively adapt the weak learners so that the expressivity of the aggregate model is gradually adapted to the data. . The idea is pretty simple. At each iteration, compute the gradient with respect to the model of the loss function evaluated at the current iteration. That gradient represents the linear approximation of the change to the model that would ensure a lower loss, a sort of gradient descent in function space (then restricted to a particular parametric form). In the case of the regression problem with MSE loss, the gradient is just the vector of the residuals, with indices being the data points of the training set. The gist of the algorithm consists in just training a new learner to match the current residuals and then to add it to the current model. Again, see [^Murphy], chapter 16, for further discussion. . Comments . The fact that the performance on a given dataset is statistically better, in terms of some suitable measure, does not mean that the model should be accepted without questions. For instance, training a random forest on a dataset representing how a quantity depends on another might lead to a model with a better performance when statistical errors are considered, but with a poor representation of the function class to which the relationship belongs (e.g. the model might produce a piecewise continuous function instead of a smooth one). . Also, boosting is a sort of midway between a parametric model and a nonparametric one, in the sense that while a parametric family of learner is chosen, the model itself is adapting to the data by expanding the space of functions that can be represented beyond what is allowed by the original weak learner. This is rather obvious from the fact that the model used is additive in the weak learners, and that each stage of the construction is designed to capture different aspects of the dataset which cannot be captured in a single pass in which all the learners are exposed to the dataset. This also explains the power of these methods. . Why is this interesting? . Apart from having used XGBoost in the past, I have been recently playing with the problem of reconstructing the geometry of the expanding universe we live in, given observational data. The goal is to have a flexible tool that is automatically adapting to the small number of data points available. GPs work well and they are currently used in some areas of Cosmology, but it is interesting to consider other options that can be used in a regime in which very few points are available and yet the desired model should be expressive enough not to inject approximations that are too brutal and not very faithful to the data. I was looking in particular at the data presented in the recent paper [^BernardoSaid], see the notebooks on GitHub here, where Gaussian processes are used to model possible gravitational theories. . It is clear that while boosted trees are certainly an option, they fail to include the necessary degree of smoothness that is implicitly required for the functions for which an interpolation is sought, and some other type of weak learner should be used. Alternatively, the choice of Gaussian Processes with suitable assumptions on the kernel is the best course of action, as it makes no commitment on the parametric form of the model, while retaining the ability of controlling smoothness of the class of functions that the model targets. . References . [^XGBoost] Chen, T. and Guestrin, C., 2016, August. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining (pp. 785-794). . [^Murphy] Murphy, Kevin P. Machine learning: a probabilistic perspective. MIT press, 2012. . [^Dietterich2000] Dietterich, Thomas G. “Ensemble methods in machine learning.” International workshop on multiple classifier systems. Springer, Berlin, Heidelberg, 2000. . [^Dietterich2002] Dietterich, Thomas G. “Ensemble learning.” The handbook of brain theory and neural networks 2 (2002): 110-125. . [^BernardoSaid] Bernardo, Reginald Christian, and Jackson Levi Said. “A data-driven Reconstruction of Horndeski gravity via the Gaussian processes.” arXiv preprint arXiv:2105.12970 (2021). (https://arxiv.org/abs/2105.12970)[https://arxiv.org/abs/2105.12970] .",
            "url": "https://lsindoni.github.io/blog/markdown/2021/06/15/boosting.html",
            "relUrl": "/markdown/2021/06/15/boosting.html",
            "date": " • Jun 15, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Intro",
            "content": "I have decided to use this space to present material that I find interesting, as a way to condense and present in an organic form what I read and learn while tinkering with mathematical models that cross my path. . There will be no specific order or explicit main thread of ideas, at least not by design. I am curious by nature, and my interests are broad. . I am also a firm believer in serendipity as a driver of discovery. Sometimes one has to simply try and see if something happens, instead of engineering discovery. Hopefully what I collect here will be useful for other people too! .",
            "url": "https://lsindoni.github.io/blog/markdown/2021/05/25/Intro.html",
            "relUrl": "/markdown/2021/05/25/Intro.html",
            "date": " • May 25, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://lsindoni.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a physicist, I worked on classical and quantum gravity problems first at SISSA/ISAS in Trieste, then as a postdoc at the Albert Einstein Institute (Max Planck Institute for gravitational Physics) in Potsdam. I worked at InveniaLabs in Cambridge (UK) as a researcher, working mainly on power grids, electricity prices and decision making under uncertainty. I worked as a model validation specialist at Deutsche Bank in Berlin. Yes, I like to learn new things. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://lsindoni.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://lsindoni.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}